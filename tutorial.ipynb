{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading the Cell Tracking Data**\n",
    "\n",
    "In this notebook, we will be using several datasets from the Cell Tracking Challenge, including:\n",
    "\n",
    "- **BF-C2DL-HSC**\n",
    "- **BF-C2DL-MuSC**\n",
    "- **DIC-C2DH-HeLa**\n",
    "- **Fluo-C2DL-Huh7**\n",
    "- **Fluo-C2DL-MSC**\n",
    "- **Fluo-N2DH-SIM+**\n",
    "- **Fluo-N2DL-HeLa**\n",
    "- **PhC-C2DH-U373**\n",
    "- **PhC-C2DL-PSC**\n",
    "\n",
    "Letâ€™s begin by choosing a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"DIC-C2DH-HeLa\"          # Change this to the dataset you want to use      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download and extract this dataset using the code snippet provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url, _extract_zip\n",
    "\n",
    "dataset_path = os.path.join(\".\", \"datasets\")\n",
    "\n",
    "if not os.path.exists(os.path.join(dataset_path, dataset_name)):\n",
    "    url = f\"http://data.celltrackingchallenge.net/training-datasets/{dataset_name}.zip\"\n",
    "\n",
    "    download_url(url, \".\") # (1)\n",
    "    _extract_zip(f\"{dataset_name}.zip\", dataset_path, None) # (2)\n",
    "    os.remove(f\"{dataset_name}.zip\") # (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script **(2)** downloads the dataset, **(3)** unzips it into the `dataset_path` directory, and **(4)** remove the downloaded zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comprises the following subdirectories:\n",
    "    \n",
    "```\n",
    "Name_of_the_dataset\n",
    "- 01\n",
    "| - t0000.tif\n",
    "| - ...\n",
    "- 01_GT\n",
    "| - SEG\n",
    "| | - man_seg0000.tif\n",
    "| | - ...\n",
    "| - TRA\n",
    "| | - man_track.txt\n",
    "| | - man_track0000.tif\n",
    "| | - ...\n",
    "- 01_ST\n",
    "| - SEG\n",
    "| | - man_seg0000.tif\n",
    "| | - ...\n",
    "- 02\n",
    "| - t0000.tif\n",
    "| - ...\n",
    "- 02_GT\n",
    "| - SEG\n",
    "| | - man_seg0000.tif\n",
    "| | - ...\n",
    "| - TRA\n",
    "| | - man_track.txt\n",
    "| | - man_track0000.tif\n",
    "| | - ...\n",
    "- 02_ST\n",
    "| - SEG\n",
    "| | - man_seg0000.tif\n",
    "| | - ...\n",
    "```\n",
    "\n",
    "The data consists of two sets of image sequences stored in the `01` and `02` folders. Each set includes segmentation masks in two quality levels: the gold-standard corpus containing human-origin reference annotations as the gold truth (`GT`) and the silver-standard corpus containing computer-origin reference annotations as the silver truth (`ST`). Furthermore, the GT folder has an additional TRA folder that holds ground-truth cell trajectories in the form of both text files and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use sequence `02` for training and sequence `01` for testing for the majority of the datasets. In both scenarios, we will use the silver-standard segmentation masks, if they are avaliable, to simulate real-world scenarios where the segmentation masks are not perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset has distinct characteristics, requiring the adjustment of specific parameters to optimize training. The table below highlights the parameters that differ across datasets:\"\n",
    "\n",
    "| Dataset           | Connectivity Radius | Window size | Dataset size | Min frame | Dataset to train| Train im path| Train seg path | Train rel path |Test im path| Test seg path | Test rel path | \n",
    "|-------------------|---------------------|-------------|--------------|-----------|-----------------|--------------|----------------|----------------|------------|---------------|----------------|\n",
    "| **BF-C2DL-HSC**   | 0.02                | 20          | 1024         | 1200      | \"BF-C2DL-HSC\"   | \"02\"         | \"02_ST\", \"SEG\" | \"02_GT\", \"TRA\" |\"01\"        | \"01_ST\", \"SEG\"| \"01_GT\", \"TRA\" |\n",
    "| **BF-C2DL-MuSC**  | 0.2                 | 15          | 1024         | 200       | \"BF-C2DL-MuSC\"  | \"02\"         | \"02_ST\", \"SEG\" | \"02_GT\", \"TRA\" |\"01\"        | \"01_ST\", \"SEG\"| \"01_GT\", \"TRA\" |\n",
    "| **DIC-C2DH-HeLa** | 0.2                 | 5           | 512          | 0         | \"DIC-C2DH-HeLa\" | \"02\"         | \"02_ST\", \"SEG\" | \"02_GT\", \"TRA\" |\"01\"        | \"01_ST\", \"SEG\"| \"01_GT\", \"TRA\" |\n",
    "| **Fluo-C2DL-Huh7**| 0.2                 | 5           | 512          | 0         | \"Fluo-C2DL-Huh7\"| \"02\"         | \"02_GT\", \"TRA\" | \"02_GT\", \"TRA\" |\"01\"        | \"01_GT\", \"TRA\"| \"01_GT\", \"TRA\" |\n",
    "| **Fluo-C2DL-MSC** | 0.2                 | 5           | 1024         | 0         | \"Fluo-C2DL-MSC\" | \"02\"         | \"02_ST\", \"SEG\" | \"02_GT\", \"TRA\" |\"01\"        | \"01_ST\", \"SEG\"| \"01_GT\", \"TRA\" |\n",
    "| **Fluo-N2DH-SIM+**| 0.2                 | 5           | 512          | 0         | \"BF-C2DL-MuSC\"  | \"02\"         | \"02_GT\", \"SEG\" | \"02_GT\", \"TRA\" |\"01\"        | \"01_GT\", \"SEG\"| \"01_GT\", \"TRA\" |\n",
    "| **Fluo-N2DL-HeLa**| 0.05                | 20          | 512          | 0         | \"Fluo-N2DL-HeLa\"| \"01\"         | \"01_ST\", \"SEG\" | \"01_GT\", \"TRA\" |\"02\"        | \"02_ST\", \"SEG\"| \"02_GT\", \"TRA\" |\n",
    "| **PhC-C2DH-U373** | 0.2                 | 5           | 512          | 0         | \"PhC-C2DH-U373\" | \"02\"         | \"02_ST\", \"SEG\" | \"02_GT\", \"TRA\" |\"01\"        | \"01_ST\", \"SEG\"| \"01_GT\", \"TRA\" |\n",
    "| **PhC-C2DL-PSC**  | 0.04                | 15          | 1024         | 0         | \"PhC-C2DL-PSC\"  | \"02\"         | \"02_ST\", \"SEG\" | \"02_GT\", \"TRA\" |\"01\"        | \"01_ST\", \"SEG\"| \"01_GT\", \"TRA\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parameters = {\n",
    "    \"BF-C2DL-HSC\":    {\n",
    "        \"connectivity_radius\": 0.02, \n",
    "        \"window_size\": 20, \n",
    "        \"dataset_size\": 1024, \n",
    "        \"min_frame\": 1200, \n",
    "        \"dataset_to_train\": \"BF-C2DL-HSC\",\n",
    "        \"train_im_path\": \"02\", \n",
    "        \"train_seg_path\": [\"02_ST\", \"SEG\"], \n",
    "        \"train_rel_path\": [\"02_GT\", \"TRA\"], \n",
    "        \"test_im_path\": \"01\", \n",
    "        \"test_seg_path\": [\"01_ST\", \"SEG\"], \n",
    "        \"test_rel_path\": [\"01_GT\", \"TRA\"]\n",
    "    },\n",
    "    \"BF-C2DL-MuSC\":   {\n",
    "        \"connectivity_radius\": 0.2,  \n",
    "        \"window_size\": 15, \n",
    "        \"dataset_size\": 1024, \n",
    "        \"min_frame\": 200,  \n",
    "        \"dataset_to_train\": \"BF-C2DL-MuSC\",\n",
    "        \"train_im_path\": \"02\", \n",
    "        \"train_seg_path\": [\"02_ST\", \"SEG\"], \n",
    "        \"train_rel_path\": [\"02_GT\", \"TRA\"], \n",
    "        \"test_im_path\": \"01\", \n",
    "        \"test_seg_path\": [\"01_ST\", \"SEG\"], \n",
    "        \"test_rel_path\": [\"01_GT\", \"TRA\"]\n",
    "    },\n",
    "    \"DIC-C2DH-HeLa\":  {\n",
    "        \"connectivity_radius\": 0.2,  \n",
    "        \"window_size\": 5,  \n",
    "        \"dataset_size\": 512,  \n",
    "        \"min_frame\": 0,    \n",
    "        \"dataset_to_train\": \"DIC-C2DH-HeLa\",\n",
    "        \"train_im_path\": \"02\", \n",
    "        \"train_seg_path\": [\"02_ST\", \"SEG\"], \n",
    "        \"train_rel_path\": [\"02_GT\", \"TRA\"], \n",
    "        \"test_im_path\": \"01\", \n",
    "        \"test_seg_path\": [\"01_ST\", \"SEG\"], \n",
    "        \"test_rel_path\": [\"01_GT\", \"TRA\"]\n",
    "    },\n",
    "    \"Fluo-C2DL-Huh7\": {\n",
    "        \"connectivity_radius\": 0.2,  \n",
    "        \"window_size\": 5,  \n",
    "        \"dataset_size\": 512,  \n",
    "        \"min_frame\": 0,    \n",
    "        \"dataset_to_train\": \"Fluo-C2DL-Huh7\",\n",
    "        \"train_im_path\": \"02\", \n",
    "        \"train_seg_path\": [\"02_GT\", \"TRA\"], \n",
    "        \"train_rel_path\": [\"02_GT\", \"TRA\"], \n",
    "        \"test_im_path\": \"01\", \n",
    "        \"test_seg_path\": [\"01_GT\", \"TRA\"], \n",
    "        \"test_rel_path\": [\"01_GT\", \"TRA\"]\n",
    "    },\n",
    "    \"Fluo-C2DL-MSC\":  {\n",
    "        \"connectivity_radius\": 0.2,  \n",
    "        \"window_size\": 5,  \n",
    "        \"dataset_size\": 1024, \n",
    "        \"min_frame\": 0,    \n",
    "        \"dataset_to_train\": \"Fluo-C2DL-MSC\",\n",
    "        \"train_im_path\": \"02\", \n",
    "        \"train_seg_path\": [\"02_ST\", \"SEG\"], \n",
    "        \"train_rel_path\": [\"02_GT\", \"TRA\"], \n",
    "        \"test_im_path\": \"01\", \n",
    "        \"test_seg_path\": [\"01_ST\", \"SEG\"], \n",
    "        \"test_rel_path\": [\"01_GT\", \"TRA\"]\n",
    "    },\n",
    "    \"Fluo-N2DH-SIM+\": {\n",
    "        \"connectivity_radius\": 0.2,  \n",
    "        \"window_size\": 5,  \n",
    "        \"dataset_size\": 512,  \n",
    "        \"min_frame\": 0,    \n",
    "        \"dataset_to_train\": \"BF-C2DL-MuSC\",\n",
    "        \"train_im_path\": \"02\", \n",
    "        \"train_seg_path\": [\"02_ST\", \"SEG\"], \n",
    "        \"train_rel_path\": [\"02_GT\", \"TRA\"], \n",
    "        \"test_im_path\": \"01\", \n",
    "        \"test_seg_path\": [\"01_GT\", \"SEG\"], \n",
    "        \"test_rel_path\": [\"01_GT\", \"TRA\"]\n",
    "    },\n",
    "    \"Fluo-N2DL-HeLa\": {\n",
    "        \"connectivity_radius\": 0.05, \n",
    "        \"window_size\": 20, \n",
    "        \"dataset_size\": 512,  \n",
    "        \"min_frame\": 0,    \n",
    "        \"dataset_to_train\": \"Fluo-N2DL-HeLa\",\n",
    "        \"train_im_path\": \"01\", \n",
    "        \"train_seg_path\": [\"01_ST\", \"SEG\"], \n",
    "        \"train_rel_path\": [\"01_GT\", \"TRA\"], \n",
    "        \"test_im_path\": \"02\", \n",
    "        \"test_seg_path\": [\"02_ST\", \"SEG\"], \n",
    "        \"test_rel_path\": [\"02_GT\", \"TRA\"]\n",
    "    },\n",
    "    \"PhC-C2DH-U373\":  {\n",
    "        \"connectivity_radius\": 0.2,  \n",
    "        \"window_size\": 5,  \n",
    "        \"dataset_size\": 512,  \n",
    "        \"min_frame\": 0,    \n",
    "        \"dataset_to_train\": \"PhC-C2DH-U373\",\n",
    "        \"train_im_path\": \"02\", \n",
    "        \"train_seg_path\": [\"02_ST\", \"SEG\"], \n",
    "        \"train_rel_path\": [\"02_GT\", \"TRA\"], \n",
    "        \"test_im_path\": \"01\", \n",
    "        \"test_seg_path\": [\"01_ST\", \"SEG\"], \n",
    "        \"test_rel_path\": [\"01_GT\", \"TRA\"]\n",
    "    },\n",
    "    \"PhC-C2DL-PSC\":   {\n",
    "        \"connectivity_radius\": 0.04, \n",
    "        \"window_size\": 15, \n",
    "        \"dataset_size\": 1024, \n",
    "        \"min_frame\": 0,    \n",
    "        \"dataset_to_train\": \"PhC-C2DL-PSC\",\n",
    "        \"train_im_path\": \"02\", \n",
    "        \"train_seg_path\": [\"02_ST\", \"SEG\"], \n",
    "        \"train_rel_path\": [\"02_GT\", \"TRA\"], \n",
    "        \"test_im_path\": \"01\", \n",
    "        \"test_seg_path\": [\"01_ST\", \"SEG\"], \n",
    "        \"test_rel_path\": [\"01_GT\", \"TRA\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def load_dataset_config(dataset_name):\n",
    "    if dataset_name not in dataset_parameters:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' is not defined.\")\n",
    "    \n",
    "    config = dataset_parameters[dataset_name]\n",
    "    print(f\"Loaded configuration for {dataset_name}:\\n{config}\\n\")\n",
    "    return config\n",
    "\n",
    "config = load_dataset_config(dataset_name)\n",
    "\n",
    "# Apply parameters\n",
    "connectivity_radius = config[\"connectivity_radius\"]\n",
    "window_size = config[\"window_size\"]\n",
    "dataset_size = config[\"dataset_size\"]\n",
    "min_frame = config[\"min_frame\"]\n",
    "dataset_to_train = config[\"dataset_to_train\"]\n",
    "train_im_path = config[\"train_im_path\"]\n",
    "train_seg_path = config[\"train_seg_path\"]\n",
    "train_rel_path = config[\"train_rel_path\"]\n",
    "test_im_path = config[\"test_im_path\"]\n",
    "test_seg_path = config[\"test_seg_path\"]\n",
    "test_rel_path = config[\"test_rel_path\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset used for training (dataset_to_train) is different from the test dataset (dataset_name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = os.path.join(\".\", \"datasets\")\n",
    "\n",
    "if not os.path.exists(os.path.join(train_dataset_path, dataset_to_train)):\n",
    "    url = f\"http://data.celltrackingchallenge.net/training-datasets/{dataset_to_train}.zip\"\n",
    "\n",
    "    download_url(url, \".\") # (1)\n",
    "    _extract_zip(f\"{dataset_to_train}.zip\", train_dataset_path, None) # (2)\n",
    "    os.remove(f\"{dataset_to_train}.zip\") # (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path = os.path.join(dataset_path, dataset_to_train, train_im_path)\n",
    "train_segmentation_path = os.path.join(dataset_path, dataset_to_train, train_seg_path[0], train_seg_path[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data using the code snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "\n",
    "def load_images(path):\n",
    "    images = []\n",
    "    for file in glob.glob(path + \"/*.tif\"): # (1)\n",
    "        image = cv2.imread(file, cv2.IMREAD_UNCHANGED) # (2)\n",
    "        images.append(image) # (3)\n",
    "\n",
    "    return images\n",
    "\n",
    "train_images = load_images(train_image_path)\n",
    "train_segmentations = load_images(train_segmentation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script **(1)** iterates over each file in the specified directory path containing a `.tif` extension, **(2)** reads the image using OpenCV's `cv2.imread()` function with the `IMREAD_UNCHANGED flag` indicating that the image should be loaded without any modification or conversion, and **(3)** appends the loaded image to the `images` list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize some frames from the training image sequence along with their corresponding segmentation mask.\n",
    "\n",
    "The script below plots a specified number of frames distributed evenly over the training image sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "number_of_frames = 5\n",
    "\n",
    "total_frames = len(train_segmentations) \n",
    "\n",
    "plot_interval = total_frames // number_of_frames \n",
    "frames_to_plot = [i for i in range(0, total_frames, plot_interval)]\n",
    "\n",
    "fig, axs = plt.subplots(2, number_of_frames + 1, figsize=(20, 6)) # (5)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "for i, frame in enumerate(frames_to_plot):\n",
    "    if i == 0:\n",
    "        axs[0, i].set_ylabel(\"Intensity image\", fontsize=16)\n",
    "        axs[1, i].set_ylabel(\"Segmentation\", fontsize=16)\n",
    "\n",
    "    axs[0, i].imshow(train_images[frame], cmap='gray')\n",
    "    axs[0, i].set_title(f\"Frame {frame}\", fontsize=16)\n",
    "    axs[0, i].tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "    # Plot segmentation\n",
    "    axs[1, i].imshow(train_segmentations[frame], cmap='tab20b')\n",
    "    axs[1, i].tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating a Graph From Segmented Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAGIK models cell motion and interactions as a directed graph, where nodes represent segmented cells and edges connect spatially close cells across frames.\n",
    "\n",
    "We will implement the `GraphFromSegmentations` class to generate a graph from the segmented video frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import measure\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class GraphFromSegmentations:\n",
    "    def __init__(self, connectivity_radius, max_frame_distance):\n",
    "        self.connectivity_radius = connectivity_radius # (1)\n",
    "        self.max_frame_distance = max_frame_distance # (2)\n",
    "\n",
    "    def __call__(self, images, segmentations, relation):\n",
    "        x, node_index_labels, frames = [], [], []\n",
    "        for frame, (image, segmentation) in enumerate(zip(images, segmentations)): # (3)\n",
    "            features, index_labels = self.compute_node_features(image, segmentation) # (4)\n",
    "\n",
    "            x.append(features) # (5)\n",
    "            node_index_labels.append(index_labels) # (6)\n",
    "            frames.append([frame] * len(features)) # (7)\n",
    "\n",
    "        x = np.concatenate(x)\n",
    "        node_index_labels = np.concatenate(node_index_labels)\n",
    "        frames = np.concatenate(frames)\n",
    "\n",
    "        edge_index, edge_attr = self.compute_connectivity(x, frames) # (8)\n",
    "        edge_ground_truth = self.compute_ground_truth( # (9)\n",
    "            node_index_labels, edge_index, relation\n",
    "        )\n",
    "\n",
    "        edge_index = edge_index.T\n",
    "        edge_attr = edge_attr[:, None]\n",
    "        edge_ground_truth = edge_ground_truth[:, None]\n",
    "\n",
    "        graph = Data( # (10)\n",
    "            x=torch.tensor(x, dtype=torch.float),\n",
    "            edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "            edge_attr=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            distance=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            frames=torch.tensor(frames, dtype=torch.float),\n",
    "            y=torch.tensor(edge_ground_truth, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def compute_node_features(self, image, segmentation):\n",
    "        labels = np.unique(segmentation)\n",
    "\n",
    "        x, indices = [], []\n",
    "        for label in labels[1:]:\n",
    "            mask = segmentation == label\n",
    "            props = measure.regionprops(mask.astype(np.int32), intensity_image=image)[0]\n",
    "\n",
    "            centroids = props.centroid / np.array(segmentation.shape)\n",
    "\n",
    "            x.append([*centroids])\n",
    "            indices.append(label)\n",
    "\n",
    "        return x, indices\n",
    "\n",
    "    def compute_connectivity(self, x, frames):\n",
    "        positions = x[:, :2]\n",
    "        distances = np.linalg.norm(positions[:, None] - positions, axis=-1)\n",
    "\n",
    "        frame_diff = (frames[:, None] - frames) * -1\n",
    "\n",
    "        mask = (distances < self.connectivity_radius) & ( \n",
    "            (frame_diff <= self.max_frame_distance) & (frame_diff > 0)\n",
    "        )\n",
    "\n",
    "        edge_index = np.argwhere(mask) \n",
    "        edge_attr = distances[mask] \n",
    "\n",
    "        return edge_index, edge_attr\n",
    "\n",
    "    def compute_ground_truth(self, indices, edge_index, relation):\n",
    "        sender = indices[edge_index[:, 0]] \n",
    "        receiver = indices[edge_index[:, 1]]\n",
    "        self_connections_mask = sender == receiver\n",
    "\n",
    "        relation_indices = relation[:, [-1, 0]] \n",
    "        relation_indices = relation_indices[relation_indices[:, 0] != 0]\n",
    "\n",
    "        relation_mask = np.zeros(len(edge_index), dtype=bool)\n",
    "        for i, (s, r) in enumerate(zip(sender, receiver)):\n",
    "            if np.any((relation_indices == [s, r]).all(1)): \n",
    "                relation_mask[i] = True\n",
    "\n",
    "        ground_truth = self_connections_mask | relation_mask\n",
    "\n",
    "        return ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GraphFromSegmentations` class is initialized with two parameters: **(1)** `connectivity_radius` and **(2)** `max_frame_distance`. These parameters play an important role in establishing the spatial and temporal thresholds necessary for determining connectivity between nodes within the graph structure.\n",
    "\n",
    "In the call method, `GraphFromSegmentations` receives two inputs: the segmented video frames (`segmentations`) and the parent-child relationships between cells (`relation`). **(3)** The method identifies separate objects in each frame of the segmented video data using their index labels. Next, **(4)** it calculates relevant features such as normalized centroids and eccentricity. **(5)** These features are stored in a set called `x`. The algorithm repeats this process for every object in the frame, creating a collection of node features (`x`), **(6)** index labels (`node_index_labels`), and **(7)** their corresponding frame numbers (`frames`), all poised for further processing.\n",
    "\n",
    "Leveraging the extracted node features, **(8)** `GraphFromSegmentations` proceeds to calculate pairwise distances between the positions of the nodes. Simultaneously, it computes the temporal difference between frames. Based on the specified thresholds (`connectivity_radius` and `max_frame_distance`), it identifies nodes that are both spatially and temporally close. The result is a set of edge indices (`edge_index`) and corresponding distances (`edge_attr`) representing the connectivity between nodes.\n",
    "\n",
    "Finally, **(9)** the ground-truth edges are computed. The generated graph includes a redundant number of edges with respect to the actual associations between cells. MAGIK aims to prune the redundant edges while retaining the true connections. Therefore, the ground truth for each edge is a binary value indicating whether an edge should connect two detections, i.e., an edge classification problem. `GraphFromSegmentations` defines the ground truth by comparing the node index labels and parent-child relationships. Firstly, it identifies self-connections where sender and receiver nodes have the same node index labels. Next, it explores the cell relationships to find relational connections, such as cell divisions. The ground truths are derived from the combination of self-connections and relational connections. \n",
    "\n",
    "**(10)** `GraphFromSegmentations` constructs a PyTorch Data object using node features, edge indices, attributes, distances, frames, and ground truth. This object encapsulates all necessary information for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the `GraphFromSegmentations` class with a connectivity radius of 0.2 (equivalent to 20% of the image size) and a maximum frame distance of 2 to reconnect cells not detected in consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_constructor = GraphFromSegmentations(connectivity_radius=connectivity_radius, max_frame_distance=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now construct the training graph using `graph_constructor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_relation_path = os.path.join(dataset_path, dataset_to_train, train_rel_path[0], train_rel_path[1]) # (1)\n",
    "train_relation = np.loadtxt(train_relation_path + \"/man_track.txt\", dtype=int)\n",
    "\n",
    "train_graph = graph_constructor(train_images, train_segmentations, train_relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Loads the `man_track.txt` file containing the parent-child relationships between cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to explore the graph data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of nodes:\", len(train_graph.x))\n",
    "print(\"Number of edges:\", len(train_graph.edge_index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i, j in train_graph.edge_index.T:\n",
    "    plt.plot(\n",
    "        [train_graph.x[i, 1], train_graph.x[j, 1]],\n",
    "        [train_graph.x[i, 0], train_graph.x[j, 0]],\n",
    "        c=\"black\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "plt.scatter(\n",
    "    train_graph.x[:, 1],\n",
    "    train_graph.x[:, 0],\n",
    "    c=train_graph.frames,\n",
    "    cmap=\"viridis\",\n",
    "    zorder=10,\n",
    ")\n",
    "# label colorbar\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_title('Frame', fontsize=14)\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatter plot depicts a graph with nodes represented as dots. The $x$ and $y$ coordinates represent the normalized node centroids. The color of each dot corresponds to the frame number, as shown on the color bar. The black lines on the plot illustrate the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building a Dataset for Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The training dataset consists of a single graph derived from the training video sequence. Although this may initially appear as limited data, it proves to be ample for effectively training the MAGIK model. To address the scarcity of data, we adopt a strategic approach of augmenting the training graph by splitting it into smaller temporal subgraphs.\n",
    "\n",
    "The `CellTracingDataset` implements this augmentation strategy by dividing the training graph into smaller subgraphs parameterized by the `window_size`parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CellTracingDataset(Dataset):\n",
    "    def __init__(self, graph, window_size, dataset_size, transform=None, min_frame=0):\n",
    "        self.graph = graph\n",
    "\n",
    "        self.window_size = window_size # (1)\n",
    "        self.dataset_size = dataset_size\n",
    "\n",
    "        frames, edge_index = graph.frames, graph.edge_index\n",
    "        self.pair_frames = torch.stack(\n",
    "            [frames[edge_index[0, :]], frames[edge_index[1, :]]], axis=1\n",
    "        )\n",
    "        self.frames = frames\n",
    "        self.max_frame = frames.max()\n",
    "\n",
    "        self.transform = transform \n",
    "\n",
    "        self.min_frame = min_frame\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_idx = np.random.randint(self.window_size + self.min_frame, self.max_frame + 1) # (2)\n",
    "\n",
    "        start_frame = frame_idx - self.window_size\n",
    "        node_mask = (self.frames >= start_frame) & (self.frames < frame_idx) # (3)\n",
    "        x = self.graph.x[node_mask] # (4)\n",
    "\n",
    "        edge_mask = (self.pair_frames >= start_frame) & (self.pair_frames < frame_idx) # (5)\n",
    "        edge_mask = edge_mask.all(axis=1) \n",
    "\n",
    "        edge_index = self.graph.edge_index[:, edge_mask] # (6)\n",
    "        edge_index -= edge_index.min() \n",
    "\n",
    "        edge_attr = self.graph.edge_attr[edge_mask] # (7)\n",
    "\n",
    "        # sample ground truth edges\n",
    "        ground_truth_edges = self.graph.y[edge_mask] # (8)\n",
    "\n",
    "        graph = Data( # (9)\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            distance=edge_attr,\n",
    "            y=ground_truth_edges,\n",
    "        )\n",
    "\n",
    "        if self.transform: # (10)\n",
    "            graph = self.transform(graph)\n",
    "\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** The `window_size` parameter determines the number of frames in each subgraph. **(2)** The dataset generates subgraphs by randomly sliding a window across the training graph. The subgraph is constructed by extracting **(3-4)** nodes and **(5-8)** edges within the window. **(9)** The dataset returns the subgraph as a PyTorch Data object. \n",
    "\n",
    "To further enhance the training dataset, additional augmentations can be applied to the subgraphs. The `CellTracingDataset` class provides the flexibility to include custom augmentations by specifying the `transform` parameter. \n",
    "\n",
    "The following code snippet defines two augmentations: `RandomRotation` and `RandomFlip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class RandomRotation: # (1)\n",
    "    def __call__(self, graph):\n",
    "        graph = graph.clone()\n",
    "        centered_features = graph.x[:, :2] - 0.5\n",
    "\n",
    "        angle = np.random.rand() * 2 * np.math.pi\n",
    "        rotation_matrix = torch.tensor(\n",
    "            [\n",
    "                [math.cos(angle), -math.sin(angle)],\n",
    "                [math.sin(angle), math.cos(angle)],\n",
    "            ]\n",
    "        )\n",
    "        rotated_features = torch.matmul(centered_features, rotation_matrix)\n",
    "\n",
    "        graph.x[:, :2] = rotated_features + 0.5\n",
    "        return graph\n",
    "    \n",
    "class RandomFlip: # (2)\n",
    "    def __call__(self, graph):\n",
    "        graph = graph.clone()\n",
    "        centered_features = graph.x[:, :2] - 0.5\n",
    "\n",
    "        if np.random.randint(2):\n",
    "            centered_features[:, 0] *= -1\n",
    "        \n",
    "        if np.random.randint(2):\n",
    "            centered_features[:, 1] *= -1\n",
    "        \n",
    "        graph.x[:, :2] = centered_features + 0.5\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** The `RandomRotation` augmentation function randomly rotates the positional features of the nodes within the subgraph. \n",
    "\n",
    "Likewise, **(2)** the `RandomFlip` augmentation randomly flips the positional features of the nodes along the $x$-axis or $y$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_dataset = CellTracingDataset(\n",
    "    train_graph,\n",
    "    window_size=window_size,\n",
    "    dataset_size=dataset_size, # (1)\n",
    "    transform=transforms.Compose([RandomRotation(), RandomFlip()]),\n",
    "    min_frame=min_frame,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** `dataset_size` controls the number of subgraphs generated from the training graph per epoch. \n",
    "\n",
    "The training dataset is instantiated with a window size of 5 frames and a dataset size of 512 subgraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining the Data Loaders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we proceed to define the data loaders, which are responsible for feeding the data to the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building MAGIK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet defines the [MAGIK](https://www.nature.com/articles/s42256-022-00595-0) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplay import GraphToEdgeMAGIK\n",
    "import torch.nn as nn\n",
    "\n",
    "model = GraphToEdgeMAGIK([96,] * 4, 1, out_activation=nn.Sigmoid) # (1)\n",
    "\n",
    "print(model) # (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Instantiates a simplified version of MAGIK, which is a message-passing neural network. The model has four layers, and each layer contains 96 hidden features. Along with the message-passing layers, the model also includes a node encoder, an edge encoder, and a classification head.  **(2)** Prints the model summary.\n",
    "\n",
    "MAGIK is similar to message-passing neural networks that we have seen in previous examples. However, the main difference is that MAGIK implements a local attention mechanism that allows the model to concentrate on specific nodes and edges during message passing. This mechanism comes into play when aggregating messages to a node. Each message's contribution has a weight that depends on the distance between the connected nodes through a function with learnable parameters defining a learnable local receptive field. With this mechanism, MAGIK can focus on relevant distance-based features during message passing, which is crucial for cell tracking tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training MAGIK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code snippet to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplay import BinaryClassifier, Adam, Trainer\n",
    "\n",
    "classifier = BinaryClassifier(model=model, optimizer=Adam(lr=1e-3))\n",
    "classifier = classifier.create()\n",
    "\n",
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(classifier, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluating MAGIK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can evaluate its performance on the test dataset. \n",
    "\n",
    "We start by loading the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = os.path.join(dataset_path, dataset_name, test_im_path)\n",
    "test_segmentation_path = os.path.join(dataset_path, dataset_name, test_seg_path[0], test_seg_path[1])\n",
    "\n",
    "test_images = load_images(test_image_path)\n",
    "test_segmentations = load_images(test_segmentation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding parent-child relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_relation_path = os.path.join(dataset_path, dataset_name, test_rel_path[0], test_rel_path[1])\n",
    "test_relation = np.loadtxt(test_relation_path + \"/man_track.txt\", dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now construct the test graph using `graph_constructor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph = graph_constructor(test_images, test_segmentations, test_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of nodes:\", len(test_graph.x))\n",
    "print(\"Number of edges:\", len(test_graph.edge_index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the test graph, we can assess the model's performance by calculating the f1-score of the predicted and ground-truth edge classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "classifier.eval()\n",
    "pred = classifier(test_graph)\n",
    "predictions = pred.detach().numpy() > 0.5\n",
    "\n",
    "ground_truth = test_graph.y\n",
    "\n",
    "score = f1_score(ground_truth, predictions)\n",
    "print(f\"Test F1 score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can expect an f1-score of approximately 0.99 on the test graph, exhibiting the model's ability to accurately predict cell temporal associations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that MAGIK does not output cell trajectories, but a graph structure that shows the connections between cells across frames. To generate cell trajectories, a post-processing algorithm is applied to the predicted graph structure.\n",
    "\n",
    "The `compute_trajectories` function below implements a simple post-processing algorithm to compute cell trajectories from MAGIK predictions. This might be refined based on edge probability to improve the results, e.g., as done in the file `post.py` in the `\\SW\\lib` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "class compute_trajectories:\n",
    "\n",
    "    def __call__(self, graph, predictions):\n",
    "        pruned_edges = self.prune_edges(graph, predictions)\n",
    "\n",
    "        pruned_graph = nx.Graph()\n",
    "        pruned_graph.add_edges_from(pruned_edges)\n",
    "\n",
    "        trajectories = list(nx.connected_components(pruned_graph))\n",
    "\n",
    "        return trajectories\n",
    "\n",
    "    def prune_edges(self, graph, predictions):\n",
    "        pruned_edges = []\n",
    "\n",
    "        frame_pairs = np.stack(\n",
    "            [graph.frames[graph.edge_index[0]], graph.frames[graph.edge_index[1]]],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        senders = np.unique(graph.edge_index[0])\n",
    "        for sender in senders: \n",
    "            sender_mask = graph.edge_index[0] == sender # (1)\n",
    "            candidate = predictions[sender_mask] == True # (2)\n",
    "\n",
    "            frame_diff = frame_pairs[sender_mask, 1] - frame_pairs[sender_mask, 0]\n",
    "            candidates_frame_diff = frame_diff[candidate]\n",
    "\n",
    "            if not np.any(candidate):\n",
    "                continue\n",
    "            else:\n",
    "                candidate_min_frame_diff = candidates_frame_diff.min()\n",
    "            \n",
    "            candidate_edge_index = graph.edge_index[:, sender_mask][ # (3)\n",
    "                :, candidate & (frame_diff == candidate_min_frame_diff)\n",
    "            ]\n",
    "            candidate_edge_index = candidate_edge_index.reshape(-1, 2)\n",
    "\n",
    "            if len(candidate_edge_index) == 1: # (4)\n",
    "                pruned_edges.append(tuple(*candidate_edge_index.numpy()))\n",
    "\n",
    "        return pruned_edges\n",
    "\n",
    "post_processor = compute_trajectories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** The algorithm starts by selecting a node in the first frame ($t=0$) and then links it to other nodes in the following frames, **(2)** using only edges labeled as \"linked\" by MAGIK. **(3)** If there are no \"linked\" edges connecting the sender node at time $t$ to any receiver nodes at time $t+1$, the algorithm checks future frames up to a maximum time delay. If no \"linked\" edges are found within this timeframe, the trajectory ends.\n",
    "\n",
    "When a sender node has two \"linked\" edges connecting it to two receiver nodes in a later frame, it's identified as a division. In this case, **(4)** the algorithm creates two new trajectories. This process repeats until all \"linked\" edges are dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = post_processor(test_graph, predictions.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we proceed to visualize the computed cell trajectories on top of the segmented video frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from skimage import measure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "size = test_segmentations[0].shape\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "list_of_colors = plt.cm.get_cmap(\"tab20b\", len(trajectories))\n",
    "np.random.shuffle(list_of_colors.colors)\n",
    "\n",
    "def update(frame):\n",
    "    ax.clear()\n",
    "    ax.imshow(test_images[frame], cmap=\"gray\")\n",
    "\n",
    "    segmentation = test_segmentations[frame]\n",
    "    labels = np.unique(segmentation)\n",
    "\n",
    "    for label in labels[1:]:\n",
    "        contour = measure.find_contours(segmentation == label, 0.5)[0]\n",
    "        ax.fill(\n",
    "            contour[:, 1],\n",
    "            contour[:, 0],\n",
    "            #color=list_of_colors(label),\n",
    "            color=\"purple\",\n",
    "            alpha=0.2,\n",
    "            linewidth=6,\n",
    "        )\n",
    "    ax.text(0, -5, f\"Frame: {frame}\", fontsize=16, color=\"black\")\n",
    "\n",
    "    for idx, t in enumerate(trajectories):\n",
    "        coordinates = test_graph.x[list(t)]\n",
    "        frames = test_graph.frames[list(t)]\n",
    "\n",
    "        coordinates_in_frame = coordinates[frames == frame]\n",
    "\n",
    "        if len(coordinates_in_frame) == 0:\n",
    "            continue\n",
    "\n",
    "        ax.scatter(coordinates_in_frame[:, 1] * size[1], coordinates_in_frame[:, 0] * size[0], color=\"purple\")\n",
    "        # ax.text(\n",
    "        #     coordinates_in_frame[0, 1] * size[1],\n",
    "        #     coordinates_in_frame[0, 0] * size[0],\n",
    "        #     str(idx),\n",
    "        #     fontsize=16,\n",
    "        #     color=\"white\",\n",
    "        # )\n",
    "\n",
    "        coordinates_previous_frames = coordinates[\n",
    "            (frames <= frame) & (frames >= frame - 10)\n",
    "        ]\n",
    "        ax.plot(\n",
    "            coordinates_previous_frames[:, 1] * size[1],\n",
    "            coordinates_previous_frames[:, 0] * size[0],\n",
    "            color=\"white\",\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            coordinates_in_frame[max(0, frame - 10) : frame, 1] * size[1],\n",
    "            coordinates_in_frame[max(0, frame - 10) : frame, 0] * size[0],\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=len(test_segmentations))\n",
    "\n",
    "#html_video = HTML(ani.to_jshtml())\n",
    "html_video = HTML(ani.to_html5_video())\n",
    "\n",
    "plt.close()\n",
    "html_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from skimage import measure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "list_of_colors = plt.cm.get_cmap(\"tab20b\", len(trajectories))\n",
    "np.random.shuffle(list_of_colors.colors)\n",
    "\n",
    "for frame in range(len(test_segmentations)):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.clear()\n",
    "    ax.imshow(test_images[frame], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    segmentation = test_segmentations[frame]\n",
    "    labels = np.unique(segmentation)\n",
    "\n",
    "    for label in labels[1:]:\n",
    "        contour = measure.find_contours(segmentation == label, 0.5)[0]\n",
    "        ax.fill(\n",
    "            contour[:, 1],\n",
    "            contour[:, 0],\n",
    "            #color=list_of_colors(label),\n",
    "            color=\"purple\",\n",
    "            alpha=0.2,\n",
    "            linewidth=6,\n",
    "        )\n",
    "    #ax.text(0, -5, f\"Frame: {frame}\", fontsize=16, color=\"black\")\n",
    "\n",
    "    for idx, t in enumerate(trajectories):\n",
    "        coordinates = test_graph.x[list(t)]\n",
    "        frames = test_graph.frames[list(t)]\n",
    "\n",
    "        coordinates_in_frame = coordinates[frames == frame]\n",
    "\n",
    "        if len(coordinates_in_frame) == 0:\n",
    "            continue\n",
    "\n",
    "        ax.scatter(coordinates_in_frame[:, 1] * size[1], coordinates_in_frame[:, 0] * size[0], color=\"purple\")\n",
    "            # ax.text(\n",
    "            #     coordinates_in_frame[0, 1] * size[1],\n",
    "            #     coordinates_in_frame[0, 0] * size[0],\n",
    "            #     str(idx),\n",
    "            #     fontsize=16,\n",
    "            #     color=\"white\",\n",
    "            # )\n",
    "\n",
    "        coordinates_previous_frames = coordinates[\n",
    "                (frames <= frame) & (frames >= frame - 10)\n",
    "        ]\n",
    "        f = frames[(frames <= frame) & (frames >= frame - 10)]\n",
    "        coordinates_previous_frames = coordinates_previous_frames[np.argsort(f[f <= frame])]\n",
    "        ax.plot(\n",
    "            coordinates_previous_frames[:, 1] * size[1],\n",
    "            coordinates_previous_frames[:, 0] * size[0],\n",
    "            color=\"white\",\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            coordinates_in_frame[max(0, frame - 10) : frame, 1] * size[1],\n",
    "            coordinates_in_frame[max(0, frame - 10) : frame, 0] * size[0],\n",
    "            color=\"red\",\n",
    "        )\n",
    "    \n",
    "    if not os.path.exists(f\"videos/{dataset_name}\"):\n",
    "        os.makedirs(f\"videos/{dataset_name}\")\n",
    "    \n",
    "    # 0000.png, 0001.png, ...\n",
    "    # transparent background\n",
    "    plt.savefig(f\"videos/{dataset_name}/frame_{frame:04d}.png\", bbox_inches=\"tight\", pad_inches=0, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read images and compile into a video\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "video_name = f'{dataset_name}.avi'\n",
    "\n",
    "images = [img for img in os.listdir(f\"videos/{dataset_name}\") if img.endswith(\".png\")]\n",
    "frame = cv2.imread(os.path.join(f\"videos/{dataset_name}\", images[0]))\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "# frame rate 30\n",
    "video = cv2.VideoWriter(os.path.join(\"videos\", video_name), 0, 5, (width,height))\n",
    "                        \n",
    "\n",
    "for image in images:\n",
    "    video.write(cv2.imread(os.path.join(f\"videos/{dataset_name}\", image)))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Saving results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_graph = Data()\n",
    "\n",
    "# Node coordinates\n",
    "predicted_graph.x = test_graph.x[:, :2]\n",
    "# Edge index\n",
    "predicted_graph.edge_index = test_graph.edge_index\n",
    "# Probabilities\n",
    "# predicted_graph.probabilities = probs\n",
    "# Predictions\n",
    "predicted_graph.prediction = predictions\n",
    "# Frames\n",
    "predicted_graph.frames = test_graph.frames\n",
    "# Ground truth\n",
    "predicted_graph.gt = test_graph.y\n",
    "\n",
    "torch.save(predicted_graph, f\"results/predicted_graph_{dataset_name}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplay_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
